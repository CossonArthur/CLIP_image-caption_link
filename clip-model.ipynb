{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/arthurcoss/clip-model-56a5c0?scriptVersionId=118593356\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport shutil\nimport random\nimport glob\nfrom PIL import Image\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,array_to_img\n\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nAUTOTUNE = tf.data.AUTOTUNE\nprint(tf.__version__)\n\n# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-08T15:44:22.246888Z","iopub.execute_input":"2023-02-08T15:44:22.247754Z","iopub.status.idle":"2023-02-08T15:44:22.26239Z","shell.execute_reply.started":"2023-02-08T15:44:22.247705Z","shell.execute_reply":"2023-02-08T15:44:22.260093Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"2.6.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000\nsequence_length = 32\n\n\nimage_size = (128,128)\npatch_size = 8  \nnum_patches = (image_size[0] // patch_size) ** 2\nprojection_dim = 64\ninput_shape = (128, 128, 3)\n\nlearning_rate = 0.0001\nepochs = 1  # In practice, use ~100 epochs\nbatch_size = 2\n\npath2img = '/kaggle/input/clip-dataset/resized_train/resized_train/'\npath2caption = '/kaggle/input/clip-dataset/caption_prediction_train.csv'","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:22.28014Z","iopub.execute_input":"2023-02-08T15:44:22.281119Z","iopub.status.idle":"2023-02-08T15:44:22.294195Z","shell.execute_reply.started":"2023-02-08T15:44:22.281077Z","shell.execute_reply":"2023-02-08T15:44:22.292715Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Image Encoder","metadata":{}},{"cell_type":"markdown","source":"What is missing?\n* -train val split\n* -a better dataloader","metadata":{}},{"cell_type":"code","source":"data_augmentation = tfk.Sequential(\n    [\n        tfkl.Normalization()\n    ],\n    name=\"data_augmentation\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:22.300398Z","iopub.execute_input":"2023-02-08T15:44:22.30279Z","iopub.status.idle":"2023-02-08T15:44:22.321131Z","shell.execute_reply.started":"2023-02-08T15:44:22.302722Z","shell.execute_reply":"2023-02-08T15:44:22.319721Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Patches(tfkl.Layer):\n    # Custom Layer to extract patches from images\n    # patch_size: size of the patches to be extracted\n    def __init__(self, patch_size, **kwargs):\n        # Initialize the layer\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n\n    def call(self, images):\n        # Extract patches from images\n        # images: 4D tensor of shape (batch_size, height, width, channels)\n        batch_size = tf.shape(images)[0]\n        \n        # Extracts patches from the images tensor and returns a 4D tensor\n        # shape (batch_size, num_patches, patch_height, patch_width, channels)\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        \n        # Get patch dimension\n        patch_dims = patches.shape[-1]\n        \n        # Reshape the patches to have shape (batch_size, num_patches, patch_height * patch_width * channels)\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:22.348732Z","iopub.execute_input":"2023-02-08T15:44:22.351108Z","iopub.status.idle":"2023-02-08T15:44:22.364443Z","shell.execute_reply.started":"2023-02-08T15:44:22.351066Z","shell.execute_reply":"2023-02-08T15:44:22.362656Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Vanilla ViT","metadata":{}},{"cell_type":"code","source":"class PatchEncoder(tfkl.Layer):\n    def __init__(self, num_patches, projection_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.projection = tfkl.Dense(units=projection_dim)\n        self.position_embedding = tfkl.Embedding(input_dim=num_patches, output_dim=projection_dim)\n\n    def call(self, patch):\n        # Project the patch into a lower-dimensional space\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        # Add position embedding to the encoded patch\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:22.371278Z","iopub.execute_input":"2023-02-08T15:44:22.374346Z","iopub.status.idle":"2023-02-08T15:44:22.386791Z","shell.execute_reply.started":"2023-02-08T15:44:22.37431Z","shell.execute_reply":"2023-02-08T15:44:22.385778Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def transformer_block(x, heads, key_dim, units, dropout_rate, name=''):\n\n    # normalize the input\n    x1 = tfkl.LayerNormalization(epsilon=1e-6)(x)\n    # apply multi-head attention to the normalized input\n    attention_output, attention_weights = tfkl.MultiHeadAttention(\n        num_heads=heads, \n        key_dim=key_dim, \n        dropout=dropout_rate, \n        name=name+'att'\n    )(x1, x1, return_attention_scores=True)\n    # apply dropout to the attention output\n    attention_output = tfkl.Dropout(dropout_rate)(attention_output)\n    # add the attention output to the original input\n    x2 = tfkl.Add()([attention_output, x1])\n    # normalize the result of the addition\n    x3 = tfkl.LayerNormalization(epsilon=1e-6)(x2)    \n    # apply a dense layer with the gelu activation function\n    x3 = tfkl.Dense(units, activation=tf.nn.gelu)(x3)\n    # apply dropout to the dense layer output\n    x3 = tfkl.Dropout(dropout_rate)(x3)\n    # add the dense layer output to the result of the addition\n    x3 = tfkl.Add()([x3, x2])\n\n    return x3, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:22.392272Z","iopub.execute_input":"2023-02-08T15:44:22.39482Z","iopub.status.idle":"2023-02-08T15:44:22.405347Z","shell.execute_reply.started":"2023-02-08T15:44:22.394785Z","shell.execute_reply":"2023-02-08T15:44:22.404339Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def get_visual_transformer(\n    blocks, \n    heads, \n    key_dim, \n    units, \n    num_patches, \n    projection_dim, \n    dropout_rate=0.33\n    ):\n    \n    inputs = tfkl.Input(shape=input_shape, name='inputs')\n    # Augment data\n    augmented = data_augmentation(inputs)\n    # Create patches\n    patches = Patches(patch_size)(augmented)\n    # Encode patches\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n    \n    for b in range(blocks):\n        encoded_patches, attention_weights = transformer_block(\n            encoded_patches, \n            heads, \n            key_dim, \n            units,\n            dropout_rate,\n            name='block'+str(b)+'_'\n        )\n    \n    # Create a [batch_size, projection_dim] tensor\n    representation = tfkl.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = tfkl.Flatten()(representation)\n    representation = tfkl.Dropout(0.5)(representation)\n    # Classify outputs\n    logits = tfkl.Dense(num_classes)(representation) #num_classes = dim of latent representation of image\n    # Create the Keras model\n    model = tfk.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:22.411147Z","iopub.execute_input":"2023-02-08T15:44:22.414073Z","iopub.status.idle":"2023-02-08T15:44:22.42855Z","shell.execute_reply.started":"2023-02-08T15:44:22.41404Z","shell.execute_reply":"2023-02-08T15:44:22.427421Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"What we want is a latent representation of an image.\n\nnow the output of the encoder is (None, 256, 64) which is (bz, num of patches, dim of embeding for each patch)\n\nwe try to  ","metadata":{}},{"cell_type":"code","source":"num_classes=64\nViT = get_visual_transformer(\n    blocks=4,\n    heads=4,\n    key_dim=projection_dim,\n    units=projection_dim,\n    projection_dim=projection_dim,\n    num_patches=num_patches\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:22.433753Z","iopub.execute_input":"2023-02-08T15:44:22.4365Z","iopub.status.idle":"2023-02-08T15:44:23.045138Z","shell.execute_reply.started":"2023-02-08T15:44:22.436432Z","shell.execute_reply":"2023-02-08T15:44:23.044105Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"ViT.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:23.046725Z","iopub.execute_input":"2023-02-08T15:44:23.047065Z","iopub.status.idle":"2023-02-08T15:44:23.061869Z","shell.execute_reply.started":"2023-02-08T15:44:23.047031Z","shell.execute_reply":"2023-02-08T15:44:23.060601Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninputs (InputLayer)             [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\ndata_augmentation (Sequential)  (None, 128, 128, 3)  7           inputs[0][0]                     \n__________________________________________________________________________________________________\npatches (Patches)               (None, None, 192)    0           data_augmentation[0][0]          \n__________________________________________________________________________________________________\npatch_encoder (PatchEncoder)    (None, 256, 64)      28736       patches[0][0]                    \n__________________________________________________________________________________________________\nlayer_normalization (LayerNorma (None, 256, 64)      128         patch_encoder[0][0]              \n__________________________________________________________________________________________________\nblock0_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization[0][0]        \n                                                                 layer_normalization[0][0]        \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 256, 64)      0           block0_att[0][0]                 \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256, 64)      0           dropout[0][0]                    \n                                                                 layer_normalization[0][0]        \n__________________________________________________________________________________________________\nlayer_normalization_1 (LayerNor (None, 256, 64)      128         add[0][0]                        \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256, 64)      4160        layer_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 256, 64)      0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 256, 64)      0           dropout_1[0][0]                  \n                                                                 add[0][0]                        \n__________________________________________________________________________________________________\nlayer_normalization_2 (LayerNor (None, 256, 64)      128         add_1[0][0]                      \n__________________________________________________________________________________________________\nblock1_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization_2[0][0]      \n                                                                 layer_normalization_2[0][0]      \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256, 64)      0           block1_att[0][0]                 \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 256, 64)      0           dropout_2[0][0]                  \n                                                                 layer_normalization_2[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_3 (LayerNor (None, 256, 64)      128         add_2[0][0]                      \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 256, 64)      4160        layer_normalization_3[0][0]      \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 256, 64)      0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 256, 64)      0           dropout_3[0][0]                  \n                                                                 add_2[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization_4 (LayerNor (None, 256, 64)      128         add_3[0][0]                      \n__________________________________________________________________________________________________\nblock2_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization_4[0][0]      \n                                                                 layer_normalization_4[0][0]      \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 256, 64)      0           block2_att[0][0]                 \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 256, 64)      0           dropout_4[0][0]                  \n                                                                 layer_normalization_4[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_5 (LayerNor (None, 256, 64)      128         add_4[0][0]                      \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 256, 64)      4160        layer_normalization_5[0][0]      \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 256, 64)      0           dense_3[0][0]                    \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 256, 64)      0           dropout_5[0][0]                  \n                                                                 add_4[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization_6 (LayerNor (None, 256, 64)      128         add_5[0][0]                      \n__________________________________________________________________________________________________\nblock3_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization_6[0][0]      \n                                                                 layer_normalization_6[0][0]      \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 256, 64)      0           block3_att[0][0]                 \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 256, 64)      0           dropout_6[0][0]                  \n                                                                 layer_normalization_6[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_7 (LayerNor (None, 256, 64)      128         add_6[0][0]                      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 256, 64)      4160        layer_normalization_7[0][0]      \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 256, 64)      0           dense_4[0][0]                    \n__________________________________________________________________________________________________\nadd_7 (Add)                     (None, 256, 64)      0           dropout_7[0][0]                  \n                                                                 add_6[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization_8 (LayerNor (None, 256, 64)      128         add_7[0][0]                      \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 16384)        0           layer_normalization_8[0][0]      \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 16384)        0           flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 64)           1048640     dropout_8[0][0]                  \n==================================================================================================\nTotal params: 1,360,647\nTrainable params: 1,360,640\nNon-trainable params: 7\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Encoder\n","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(tfkl.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        # Embedding layer for the token\n        self.token_emb = tfkl.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        # Embedding layer for the position\n        self.pos_emb = tfkl.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        # Find the maximum length of the input\n        maxlen = tf.shape(x)[-1]\n        # Create a tensor with positions from 0 to maxlen-1\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        # Embed the positions\n        positions = self.pos_emb(positions)\n        # Embed the tokens\n        x = self.token_emb(x)\n        # Add the token and position embeddings\n        return x + positions\n\nclass TransformerEncoderBlock(tfkl.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.att = tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tfk.Sequential(\n            [\n                tfkl.Dense(ff_dim, activation=\"relu\"), \n                tfkl.Dense(embed_dim)\n            ]\n        )\n        self.layernorm1 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tfkl.Dropout(rate)\n        self.dropout2 = tfkl.Dropout(rate)\n        \n    def call(self, inputs, training):\n        # Self-attention\n        attn_output = self.att(inputs, inputs)\n        # Apply dropout to the attention output\n        attn_output = self.dropout1(attn_output, training=training)\n        # Add the attention output to the input and normalize\n        out1 = self.layernorm1(inputs + attn_output)\n        # Feed-forward\n        ffn_output = self.ffn(out1)\n        # Apply dropout to the feed-forward output\n        ffn_output = self.dropout2(ffn_output, training=training)\n        # Add the feed-forward output to the previous output and normalize\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:23.065772Z","iopub.execute_input":"2023-02-08T15:44:23.066733Z","iopub.status.idle":"2023-02-08T15:44:23.079454Z","shell.execute_reply.started":"2023-02-08T15:44:23.066668Z","shell.execute_reply":"2023-02-08T15:44:23.078397Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"embed_dim = 64 # Embedding size for each token\nlatent_dim = 1024 # Dimention of the latent space\nnum_heads = 4 # Number of attention heads\n\nencoder_inputs = tfk.Input(shape=(sequence_length), dtype=\"int64\", name=\"encoder_inputs\")\n# Adding token and position embedding layer\nx = TokenAndPositionEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n# Adding transformer encoder block\nencoder_outputs = TransformerEncoderBlock(embed_dim, num_heads, latent_dim)(x)\n# Defining the encoder model\n# Apply global average pooling\nx = tfkl.GlobalAveragePooling1D()(encoder_outputs)\n# Apply dropout\nx = tfkl.Dropout(0.5)(x)\n# Add a dense layer with softmax activation\noutputs = tfkl.Dense(64, activation=\"relu\")(x)\nencoder_text = tfk.Model(encoder_inputs, outputs)\n# Print the summary of the encoder model\nencoder_text.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:44:23.08292Z","iopub.execute_input":"2023-02-08T15:44:23.083298Z","iopub.status.idle":"2023-02-08T15:44:23.3482Z","shell.execute_reply.started":"2023-02-08T15:44:23.083271Z","shell.execute_reply":"2023-02-08T15:44:23.34703Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nencoder_inputs (InputLayer)  [(None, 32)]              0         \n_________________________________________________________________\ntoken_and_position_embedding (None, 32, 64)            1282048   \n_________________________________________________________________\ntransformer_encoder_block (T (None, 32, 64)            198784    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 64)                4160      \n=================================================================\nTotal params: 1,484,992\nTrainable params: 1,484,992\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"class CLIP(tfk.Model):\n    def __init__(self, textEncoder, imageEncoder):\n        super().__init__()\n        self.textEncoder = textEncoder\n        self.imageEncoder = imageEncoder\n\n    def compile(self, optimizer):\n        super().compile()\n        self.optimizer = optimizer\n        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n        \n    @property\n    def metrics(self):\n        return [self.loss_tracker]        \n    \n    def call(self, features, training=False):\n        with tf.device(\"/gpu:0\"):\n            caption_embeddings = self.textEncoder(features[1], training=training)\n        with tf.device(\"/gpu:1\"):\n            image_embeddings = self.imageEncoder(features[0], training=training)\n        return caption_embeddings, image_embeddings\n    \n    def compute_loss(self, caption_embeddings, image_embeddings):\n        # logits[i][j] is the dot_similarity(caption_i, image_j).\n        logits = (\n            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n        )\n        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n        images_similarity = tf.matmul(\n            image_embeddings, image_embeddings, transpose_b=True\n        )\n        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n        captions_similarity = tf.matmul(\n            caption_embeddings, caption_embeddings, transpose_b=True\n        )\n        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n        targets = tfk.activations.softmax(captions_similarity + images_similarity)\n        # Compute the loss for the captions using crossentropy\n        captions_loss = tfk.losses.categorical_crossentropy(\n            y_true=targets, y_pred=logits, from_logits=True\n        )\n        # Compute the loss for the images using crossentropy\n        images_loss = tfk.losses.categorical_crossentropy(\n            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n        )\n        # Return the mean of the loss over the batch.\n        return (captions_loss + images_loss) / 2\n\n    def train_step(self, images_captions):\n        with tf.GradientTape() as tape:\n            # Forward pass\n            caption_embeddings, image_embeddings = self(images_captions, training=True)\n            loss = self.compute_loss(caption_embeddings, image_embeddings)\n        # Backward pass\n        \n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        \n        self.loss_tracker.update_state(loss)\n        \n        return {\"loss\": self.loss_tracker.result()}\n    \n    def test_step(self, images_captions):\n        caption_embeddings, image_embeddings = self(images_captions, training=False)\n        loss = self.compute_loss(caption_embeddings, image_embeddings)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:14:32.174698Z","iopub.execute_input":"2023-02-08T16:14:32.175062Z","iopub.status.idle":"2023-02-08T16:14:32.193246Z","shell.execute_reply.started":"2023-02-08T16:14:32.175033Z","shell.execute_reply":"2023-02-08T16:14:32.192013Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre processing","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(path2caption,header=0, sep='\\t')\n\nvectorise = tfkl.TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n)\nvectorise.adapt(data['caption'])\n\ndef decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, image_size)\n    return img\n\ndef process_input(links,captions):\n    return decode_and_resize(path2img+links+'.jpg'), captions\n    \ndef make_dataset(data):   \n    dataset = tf.data.Dataset.from_tensor_slices((data['ID'], vectorise(data['caption'])))\n    dataset = dataset.shuffle(batch_size * 8)\n    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n    \n    return dataset\n\n\ndata = data.iloc[:100]\ntrain_dataset = make_dataset(data.iloc[:75])\nvalid_dataset = make_dataset(data.iloc[75:100])\n\n# IMG_train, IMG_test, Cap_train, Cap_test = train_test_split(data['image'].to_numpy(),\n        #captions, test_size=0.33, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T15:51:04.020901Z","iopub.execute_input":"2023-02-08T15:51:04.021293Z","iopub.status.idle":"2023-02-08T15:51:07.811699Z","shell.execute_reply.started":"2023-02-08T15:51:04.021259Z","shell.execute_reply":"2023-02-08T15:51:07.810533Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Tensor(\"args_1:0\", shape=(32,), dtype=int64)\nTensor(\"args_1:0\", shape=(32,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"contrast_model = CLIP(encoder_text,ViT)\n\nreduce_lr = tfk.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.2, patience=3\n)\nearly_stopping = tfk.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)\noptimizer = tfk.optimizers.Adam(learning_rate)\n\ncontrast_model.compile(optimizer)\n\n\ncontrast_model.fit(train_dataset,\n    epochs=epochs,\n    validation_data=valid_dataset,\n    callbacks=[reduce_lr, early_stopping],\n)\n\ncontrast_model.history()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:14:35.892708Z","iopub.execute_input":"2023-02-08T16:14:35.893101Z","iopub.status.idle":"2023-02-08T16:14:46.796602Z","shell.execute_reply.started":"2023-02-08T16:14:35.893068Z","shell.execute_reply":"2023-02-08T16:14:46.794329Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"38/38 [==============================] - 11s 75ms/step - loss: 2.8798 - val_loss: 1.6730\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3251120868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcontrast_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'History' object is not callable"],"ename":"TypeError","evalue":"'History' object is not callable","output_type":"error"}]}]}