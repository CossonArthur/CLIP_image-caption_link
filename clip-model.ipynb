{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/arthurcoss/clip-model?scriptVersionId=118595648\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport shutil\nimport random\nimport glob\nfrom PIL import Image\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nAUTOTUNE = tf.data.AUTOTUNE\nprint(tf.__version__)\n\n# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-08T16:37:58.356964Z","iopub.execute_input":"2023-02-08T16:37:58.357352Z","iopub.status.idle":"2023-02-08T16:37:59.229274Z","shell.execute_reply.started":"2023-02-08T16:37:58.357321Z","shell.execute_reply":"2023-02-08T16:37:59.228255Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.6.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000\nsequence_length = 32\n\n\nimage_size = (128,128)\npatch_size = 8  \nnum_patches = (image_size[0] // patch_size) ** 2\nprojection_dim = 64\ninput_shape = (128, 128, 3)\n\nlearning_rate = 0.001\nepochs = 100  # In practice, use ~100 epochs\nbatch_size = 5\n\npath2img = '/kaggle/input/clip-dataset/resized_train/resized_train/'\npath2caption = '/kaggle/input/clip-dataset/caption_prediction_train.csv'","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:37:59.231148Z","iopub.execute_input":"2023-02-08T16:37:59.231982Z","iopub.status.idle":"2023-02-08T16:37:59.240225Z","shell.execute_reply.started":"2023-02-08T16:37:59.231944Z","shell.execute_reply":"2023-02-08T16:37:59.237655Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Image Encoder","metadata":{}},{"cell_type":"markdown","source":"What is missing?\n* -train val split\n* -a better dataloader","metadata":{}},{"cell_type":"code","source":"data_augmentation = tfk.Sequential(\n    [\n        tfkl.Normalization()\n    ],\n    name=\"data_augmentation\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:37:59.241537Z","iopub.execute_input":"2023-02-08T16:37:59.241976Z","iopub.status.idle":"2023-02-08T16:38:03.472846Z","shell.execute_reply.started":"2023-02-08T16:37:59.24193Z","shell.execute_reply":"2023-02-08T16:38:03.471933Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2023-02-08 16:37:59.410378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.411341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.535832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.536656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.537431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.538157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.539682: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-08 16:37:59.814985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.815849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.816568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.817249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.817922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:37:59.818615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:38:03.138816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:38:03.139702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:38:03.140409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:38:03.141109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:38:03.141821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:38:03.142508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2023-02-08 16:38:03.145805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-02-08 16:38:03.146605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}]},{"cell_type":"code","source":"class Patches(tfkl.Layer):\n    # Custom Layer to extract patches from images\n    # patch_size: size of the patches to be extracted\n    def __init__(self, patch_size, **kwargs):\n        # Initialize the layer\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n\n    def call(self, images):\n        # Extract patches from images\n        # images: 4D tensor of shape (batch_size, height, width, channels)\n        batch_size = tf.shape(images)[0]\n        \n        # Extracts patches from the images tensor and returns a 4D tensor\n        # shape (batch_size, num_patches, patch_height, patch_width, channels)\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        \n        # Get patch dimension\n        patch_dims = patches.shape[-1]\n        \n        # Reshape the patches to have shape (batch_size, num_patches, patch_height * patch_width * channels)\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:03.47559Z","iopub.execute_input":"2023-02-08T16:38:03.475953Z","iopub.status.idle":"2023-02-08T16:38:03.483608Z","shell.execute_reply.started":"2023-02-08T16:38:03.47591Z","shell.execute_reply":"2023-02-08T16:38:03.482351Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Vanilla ViT","metadata":{}},{"cell_type":"code","source":"class PatchEncoder(tfkl.Layer):\n    def __init__(self, num_patches, projection_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.projection = tfkl.Dense(units=projection_dim)\n        self.position_embedding = tfkl.Embedding(input_dim=num_patches, output_dim=projection_dim)\n\n    def call(self, patch):\n        # Project the patch into a lower-dimensional space\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        # Add position embedding to the encoded patch\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:03.485437Z","iopub.execute_input":"2023-02-08T16:38:03.485881Z","iopub.status.idle":"2023-02-08T16:38:03.500782Z","shell.execute_reply.started":"2023-02-08T16:38:03.485845Z","shell.execute_reply":"2023-02-08T16:38:03.499744Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def transformer_block(x, heads, key_dim, units, dropout_rate, name=''):\n\n    # normalize the input\n    x1 = tfkl.LayerNormalization(epsilon=1e-6)(x)\n    # apply multi-head attention to the normalized input\n    attention_output, attention_weights = tfkl.MultiHeadAttention(\n        num_heads=heads, \n        key_dim=key_dim, \n        dropout=dropout_rate, \n        name=name+'att'\n    )(x1, x1, return_attention_scores=True)\n    # apply dropout to the attention output\n    attention_output = tfkl.Dropout(dropout_rate)(attention_output)\n    # add the attention output to the original input\n    x2 = tfkl.Add()([attention_output, x1])\n    # normalize the result of the addition\n    x3 = tfkl.LayerNormalization(epsilon=1e-6)(x2)    \n    # apply a dense layer with the gelu activation function\n    x3 = tfkl.Dense(units, activation=tf.nn.gelu)(x3)\n    # apply dropout to the dense layer output\n    x3 = tfkl.Dropout(dropout_rate)(x3)\n    # add the dense layer output to the result of the addition\n    x3 = tfkl.Add()([x3, x2])\n\n    return x3, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:03.502847Z","iopub.execute_input":"2023-02-08T16:38:03.503146Z","iopub.status.idle":"2023-02-08T16:38:03.513591Z","shell.execute_reply.started":"2023-02-08T16:38:03.50312Z","shell.execute_reply":"2023-02-08T16:38:03.512671Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_visual_transformer(\n    blocks, \n    heads, \n    key_dim, \n    units, \n    num_patches, \n    projection_dim, \n    dropout_rate=0.33\n    ):\n    \n    inputs = tfkl.Input(shape=input_shape, name='inputs')\n    # Augment data\n    augmented = data_augmentation(inputs)\n    # Create patches\n    patches = Patches(patch_size)(augmented)\n    # Encode patches\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n    \n    for b in range(blocks):\n        encoded_patches, attention_weights = transformer_block(\n            encoded_patches, \n            heads, \n            key_dim, \n            units,\n            dropout_rate,\n            name='block'+str(b)+'_'\n        )\n    \n    # Create a [batch_size, projection_dim] tensor\n    representation = tfkl.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = tfkl.Flatten()(representation)\n    representation = tfkl.Dropout(0.5)(representation)\n    # Classify outputs\n    logits = tfkl.Dense(num_classes)(representation) #num_classes = dim of latent representation of image\n    # Create the Keras model\n    model = tfk.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:03.515084Z","iopub.execute_input":"2023-02-08T16:38:03.51547Z","iopub.status.idle":"2023-02-08T16:38:03.530166Z","shell.execute_reply.started":"2023-02-08T16:38:03.515437Z","shell.execute_reply":"2023-02-08T16:38:03.52927Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"What we want is a latent representation of an image.\n\nnow the output of the encoder is (None, 256, 64) which is (bz, num of patches, dim of embeding for each patch)\n\nwe try to  ","metadata":{}},{"cell_type":"code","source":"num_classes=64\nvisual_encoder = get_visual_transformer(\n    blocks=4,\n    heads=4,\n    key_dim=projection_dim,\n    units=projection_dim,\n    projection_dim=projection_dim,\n    num_patches=num_patches\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:03.531432Z","iopub.execute_input":"2023-02-08T16:38:03.531846Z","iopub.status.idle":"2023-02-08T16:38:04.18733Z","shell.execute_reply.started":"2023-02-08T16:38:03.531811Z","shell.execute_reply":"2023-02-08T16:38:04.186293Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"visual_encoder.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:04.188809Z","iopub.execute_input":"2023-02-08T16:38:04.189165Z","iopub.status.idle":"2023-02-08T16:38:04.205084Z","shell.execute_reply.started":"2023-02-08T16:38:04.189128Z","shell.execute_reply":"2023-02-08T16:38:04.203722Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninputs (InputLayer)             [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\ndata_augmentation (Sequential)  (None, 128, 128, 3)  7           inputs[0][0]                     \n__________________________________________________________________________________________________\npatches (Patches)               (None, None, 192)    0           data_augmentation[0][0]          \n__________________________________________________________________________________________________\npatch_encoder (PatchEncoder)    (None, 256, 64)      28736       patches[0][0]                    \n__________________________________________________________________________________________________\nlayer_normalization (LayerNorma (None, 256, 64)      128         patch_encoder[0][0]              \n__________________________________________________________________________________________________\nblock0_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization[0][0]        \n                                                                 layer_normalization[0][0]        \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 256, 64)      0           block0_att[0][0]                 \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256, 64)      0           dropout[0][0]                    \n                                                                 layer_normalization[0][0]        \n__________________________________________________________________________________________________\nlayer_normalization_1 (LayerNor (None, 256, 64)      128         add[0][0]                        \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256, 64)      4160        layer_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 256, 64)      0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 256, 64)      0           dropout_1[0][0]                  \n                                                                 add[0][0]                        \n__________________________________________________________________________________________________\nlayer_normalization_2 (LayerNor (None, 256, 64)      128         add_1[0][0]                      \n__________________________________________________________________________________________________\nblock1_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization_2[0][0]      \n                                                                 layer_normalization_2[0][0]      \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256, 64)      0           block1_att[0][0]                 \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 256, 64)      0           dropout_2[0][0]                  \n                                                                 layer_normalization_2[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_3 (LayerNor (None, 256, 64)      128         add_2[0][0]                      \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 256, 64)      4160        layer_normalization_3[0][0]      \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 256, 64)      0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 256, 64)      0           dropout_3[0][0]                  \n                                                                 add_2[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization_4 (LayerNor (None, 256, 64)      128         add_3[0][0]                      \n__________________________________________________________________________________________________\nblock2_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization_4[0][0]      \n                                                                 layer_normalization_4[0][0]      \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 256, 64)      0           block2_att[0][0]                 \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 256, 64)      0           dropout_4[0][0]                  \n                                                                 layer_normalization_4[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_5 (LayerNor (None, 256, 64)      128         add_4[0][0]                      \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 256, 64)      4160        layer_normalization_5[0][0]      \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 256, 64)      0           dense_3[0][0]                    \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 256, 64)      0           dropout_5[0][0]                  \n                                                                 add_4[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization_6 (LayerNor (None, 256, 64)      128         add_5[0][0]                      \n__________________________________________________________________________________________________\nblock3_att (MultiHeadAttention) ((None, 256, 64), (N 66368       layer_normalization_6[0][0]      \n                                                                 layer_normalization_6[0][0]      \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 256, 64)      0           block3_att[0][0]                 \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 256, 64)      0           dropout_6[0][0]                  \n                                                                 layer_normalization_6[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_7 (LayerNor (None, 256, 64)      128         add_6[0][0]                      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 256, 64)      4160        layer_normalization_7[0][0]      \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 256, 64)      0           dense_4[0][0]                    \n__________________________________________________________________________________________________\nadd_7 (Add)                     (None, 256, 64)      0           dropout_7[0][0]                  \n                                                                 add_6[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization_8 (LayerNor (None, 256, 64)      128         add_7[0][0]                      \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 16384)        0           layer_normalization_8[0][0]      \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 16384)        0           flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 64)           1048640     dropout_8[0][0]                  \n==================================================================================================\nTotal params: 1,360,647\nTrainable params: 1,360,640\nNon-trainable params: 7\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Encoder\n","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(tfkl.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        # Embedding layer for the token\n        self.token_emb = tfkl.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        # Embedding layer for the position\n        self.pos_emb = tfkl.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        # Find the maximum length of the input\n        maxlen = tf.shape(x)[-1]\n        # Create a tensor with positions from 0 to maxlen-1\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        # Embed the positions\n        positions = self.pos_emb(positions)\n        # Embed the tokens\n        x = self.token_emb(x)\n        # Add the token and position embeddings\n        return x + positions\n\nclass TransformerEncoderBlock(tfkl.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.att = tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tfk.Sequential(\n            [\n                tfkl.Dense(ff_dim, activation=\"relu\"), \n                tfkl.Dense(embed_dim)\n            ]\n        )\n        self.layernorm1 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tfkl.Dropout(rate)\n        self.dropout2 = tfkl.Dropout(rate)\n        \n    def call(self, inputs, training):\n        # Self-attention\n        attn_output = self.att(inputs, inputs)\n        # Apply dropout to the attention output\n        attn_output = self.dropout1(attn_output, training=training)\n        # Add the attention output to the input and normalize\n        out1 = self.layernorm1(inputs + attn_output)\n        # Feed-forward\n        ffn_output = self.ffn(out1)\n        # Apply dropout to the feed-forward output\n        ffn_output = self.dropout2(ffn_output, training=training)\n        # Add the feed-forward output to the previous output and normalize\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:04.209671Z","iopub.execute_input":"2023-02-08T16:38:04.210062Z","iopub.status.idle":"2023-02-08T16:38:04.223371Z","shell.execute_reply.started":"2023-02-08T16:38:04.210032Z","shell.execute_reply":"2023-02-08T16:38:04.222337Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"embed_dim = 64 # Embedding size for each token\nlatent_dim = 1024 # Dimention of the latent space\nnum_heads = 4 # Number of attention heads\n\nencoder_inputs = tfk.Input(shape=(sequence_length), dtype=\"int64\", name=\"encoder_inputs\")\n# Adding token and position embedding layer\nx = TokenAndPositionEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n# Adding transformer encoder block\nencoder_outputs = TransformerEncoderBlock(embed_dim, num_heads, latent_dim)(x)\n# Defining the encoder model\n# Apply global average pooling\nx = tfkl.GlobalAveragePooling1D()(encoder_outputs)\n# Apply dropout\nx = tfkl.Dropout(0.5)(x)\n# Add a dense layer with softmax activation\noutputs = tfkl.Dense(64, activation=\"relu\")(x)\ntext_encoder = tfk.Model(encoder_inputs, outputs)\n# Print the summary of the encoder model\ntext_encoder.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:04.225182Z","iopub.execute_input":"2023-02-08T16:38:04.225612Z","iopub.status.idle":"2023-02-08T16:38:04.481225Z","shell.execute_reply.started":"2023-02-08T16:38:04.225569Z","shell.execute_reply":"2023-02-08T16:38:04.480219Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nencoder_inputs (InputLayer)  [(None, 32)]              0         \n_________________________________________________________________\ntoken_and_position_embedding (None, 32, 64)            1282048   \n_________________________________________________________________\ntransformer_encoder_block (T (None, 32, 64)            198784    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 64)                0         \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 64)                4160      \n=================================================================\nTotal params: 1,484,992\nTrainable params: 1,484,992\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"class CLIP(tfk.Model):\n    def __init__(self, textEncoder, imageEncoder):\n        super().__init__()\n        self.textEncoder = textEncoder\n        self.imageEncoder = imageEncoder\n\n    def compile(self, optimizer):\n        super().compile()\n        self.optimizer = optimizer\n        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n        \n    @property\n    def metrics(self):\n        return [self.loss_tracker]        \n    \n    def call(self, features, training=False):\n        with tf.device(\"/gpu:0\"):\n            caption_embeddings = self.textEncoder(features[1], training=training)\n        with tf.device(\"/gpu:1\"):\n            image_embeddings = self.imageEncoder(features[0], training=training)\n        return caption_embeddings, image_embeddings\n    \n    def compute_loss(self, caption_embeddings, image_embeddings):\n        # logits[i][j] is the dot_similarity(caption_i, image_j).\n        logits = (\n            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n        )\n        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n        images_similarity = tf.matmul(\n            image_embeddings, image_embeddings, transpose_b=True\n        )\n        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n        captions_similarity = tf.matmul(\n            caption_embeddings, caption_embeddings, transpose_b=True\n        )\n        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n        targets = tfk.activations.softmax(captions_similarity + images_similarity)\n        # Compute the loss for the captions using crossentropy\n        captions_loss = tfk.losses.categorical_crossentropy(\n            y_true=targets, y_pred=logits, from_logits=True\n        )\n        # Compute the loss for the images using crossentropy\n        images_loss = tfk.losses.categorical_crossentropy(\n            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n        )\n        # Return the mean of the loss over the batch.\n        return (captions_loss + images_loss) / 2\n\n    def train_step(self, images_captions):\n        with tf.GradientTape() as tape:\n            # Forward pass\n            caption_embeddings, image_embeddings = self(images_captions, training=True)\n            loss = self.compute_loss(caption_embeddings, image_embeddings)\n        # Backward pass\n        \n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        \n        self.loss_tracker.update_state(loss)\n        \n        return {\"loss\": self.loss_tracker.result()}\n    \n    def test_step(self, images_captions):\n        caption_embeddings, image_embeddings = self(images_captions, training=False)\n        loss = self.compute_loss(caption_embeddings, image_embeddings)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:04.483098Z","iopub.execute_input":"2023-02-08T16:38:04.483473Z","iopub.status.idle":"2023-02-08T16:38:04.496664Z","shell.execute_reply.started":"2023-02-08T16:38:04.483435Z","shell.execute_reply":"2023-02-08T16:38:04.495498Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre processing","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(path2caption,header=0, sep='\\t')\n\nvectorise = tfkl.TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n)\nvectorise.adapt(data['caption'])\n\ndef decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, image_size)\n    return img\n\ndef process_input(links,captions):\n    return decode_and_resize(path2img+links+'.jpg'), captions\n    \ndef make_dataset(data):   \n    dataset = tf.data.Dataset.from_tensor_slices((data['ID'], vectorise(data['caption'])))\n    dataset = dataset.shuffle(batch_size * 8)\n    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n    \n    return dataset\n\n\ndata = data.iloc[:100]\ntrain_dataset = make_dataset(data.iloc[:75])\nvalid_dataset = make_dataset(data.iloc[75:100])","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:38:04.498082Z","iopub.execute_input":"2023-02-08T16:38:04.499021Z","iopub.status.idle":"2023-02-08T16:38:08.473684Z","shell.execute_reply.started":"2023-02-08T16:38:04.498982Z","shell.execute_reply":"2023-02-08T16:38:08.472725Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2023-02-08 16:38:05.039633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"contrast_model = CLIP(text_encoder,visual_encoder)\n\nreduce_lr = tfk.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.2, patience=3\n)\nearly_stopping = tfk.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)\noptimizer = tfk.optimizers.Adam(learning_rate)\n\ncontrast_model.compile(optimizer)\n\n\nhistory = contrast_model.fit(train_dataset,\n    epochs=epochs,\n    validation_data=valid_dataset,\n    callbacks=[reduce_lr, early_stopping],\n)\n\nvisual_encoder.save(\"vision_encoder\")\ntext_encoder.save(\"text_encoder\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:42:45.120295Z","iopub.execute_input":"2023-02-08T16:42:45.120667Z","iopub.status.idle":"2023-02-08T16:43:08.040191Z","shell.execute_reply.started":"2023-02-08T16:42:45.120637Z","shell.execute_reply":"2023-02-08T16:43:08.039186Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/100\n15/15 [==============================] - 6s 92ms/step - loss: 10.3180 - val_loss: 1.7562\nEpoch 2/100\n15/15 [==============================] - 1s 45ms/step - loss: 10.2169 - val_loss: 1.6094\nEpoch 3/100\n15/15 [==============================] - 1s 46ms/step - loss: 6.9582 - val_loss: 1.6094\nEpoch 4/100\n15/15 [==============================] - 1s 44ms/step - loss: 6.1231 - val_loss: 1.6094\nEpoch 5/100\n15/15 [==============================] - 1s 45ms/step - loss: 7.1426 - val_loss: 1.6094\nEpoch 6/100\n15/15 [==============================] - 1s 63ms/step - loss: 4.6943 - val_loss: 1.6094\nEpoch 7/100\n15/15 [==============================] - 1s 45ms/step - loss: 4.3348 - val_loss: 1.6094\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"train\", \"valid\"], loc=\"upper right\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:43:08.047818Z","iopub.execute_input":"2023-02-08T16:43:08.048821Z","iopub.status.idle":"2023-02-08T16:43:08.281081Z","shell.execute_reply.started":"2023-02-08T16:43:08.048776Z","shell.execute_reply":"2023-02-08T16:43:08.280217Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}