{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/arthurcoss/clip-model-56a5c0?scriptVersionId=118594569\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport shutil\nimport random\nimport glob\nfrom PIL import Image\nimport pandas as pd\nimport matplot.pyplot as plt\n\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nAUTOTUNE = tf.data.AUTOTUNE\nprint(tf.__version__)\n\n# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-08T16:28:39.664195Z","iopub.execute_input":"2023-02-08T16:28:39.664583Z","iopub.status.idle":"2023-02-08T16:28:39.713144Z","shell.execute_reply.started":"2023-02-08T16:28:39.664549Z","shell.execute_reply":"2023-02-08T16:28:39.711941Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"2.6.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000\nsequence_length = 32\n\n\nimage_size = (128,128)\npatch_size = 8  \nnum_patches = (image_size[0] // patch_size) ** 2\nprojection_dim = 64\ninput_shape = (128, 128, 3)\n\nlearning_rate = 0.001\nepochs = 100  # In practice, use ~100 epochs\nbatch_size = 5\n\npath2img = '/kaggle/input/clip-dataset/resized_train/resized_train/'\npath2caption = '/kaggle/input/clip-dataset/caption_prediction_train.csv'","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:28:39.718037Z","iopub.execute_input":"2023-02-08T16:28:39.719099Z","iopub.status.idle":"2023-02-08T16:28:39.725626Z","shell.execute_reply.started":"2023-02-08T16:28:39.719058Z","shell.execute_reply":"2023-02-08T16:28:39.724695Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# Image Encoder","metadata":{}},{"cell_type":"markdown","source":"What is missing?\n* -train val split\n* -a better dataloader","metadata":{}},{"cell_type":"code","source":"data_augmentation = tfk.Sequential(\n    [\n        tfkl.Normalization()\n    ],\n    name=\"data_augmentation\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:28:39.727168Z","iopub.execute_input":"2023-02-08T16:28:39.727594Z","iopub.status.idle":"2023-02-08T16:28:39.746002Z","shell.execute_reply.started":"2023-02-08T16:28:39.727556Z","shell.execute_reply":"2023-02-08T16:28:39.744926Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"class Patches(tfkl.Layer):\n    # Custom Layer to extract patches from images\n    # patch_size: size of the patches to be extracted\n    def __init__(self, patch_size, **kwargs):\n        # Initialize the layer\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n\n    def call(self, images):\n        # Extract patches from images\n        # images: 4D tensor of shape (batch_size, height, width, channels)\n        batch_size = tf.shape(images)[0]\n        \n        # Extracts patches from the images tensor and returns a 4D tensor\n        # shape (batch_size, num_patches, patch_height, patch_width, channels)\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        \n        # Get patch dimension\n        patch_dims = patches.shape[-1]\n        \n        # Reshape the patches to have shape (batch_size, num_patches, patch_height * patch_width * channels)\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:28:39.769681Z","iopub.execute_input":"2023-02-08T16:28:39.769977Z","iopub.status.idle":"2023-02-08T16:28:39.7774Z","shell.execute_reply.started":"2023-02-08T16:28:39.76995Z","shell.execute_reply":"2023-02-08T16:28:39.776325Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Vanilla ViT","metadata":{}},{"cell_type":"code","source":"class PatchEncoder(tfkl.Layer):\n    def __init__(self, num_patches, projection_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.projection = tfkl.Dense(units=projection_dim)\n        self.position_embedding = tfkl.Embedding(input_dim=num_patches, output_dim=projection_dim)\n\n    def call(self, patch):\n        # Project the patch into a lower-dimensional space\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        # Add position embedding to the encoded patch\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:28:39.779389Z","iopub.execute_input":"2023-02-08T16:28:39.780472Z","iopub.status.idle":"2023-02-08T16:28:39.788868Z","shell.execute_reply.started":"2023-02-08T16:28:39.780435Z","shell.execute_reply":"2023-02-08T16:28:39.787842Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def transformer_block(x, heads, key_dim, units, dropout_rate, name=''):\n\n    # normalize the input\n    x1 = tfkl.LayerNormalization(epsilon=1e-6)(x)\n    # apply multi-head attention to the normalized input\n    attention_output, attention_weights = tfkl.MultiHeadAttention(\n        num_heads=heads, \n        key_dim=key_dim, \n        dropout=dropout_rate, \n        name=name+'att'\n    )(x1, x1, return_attention_scores=True)\n    # apply dropout to the attention output\n    attention_output = tfkl.Dropout(dropout_rate)(attention_output)\n    # add the attention output to the original input\n    x2 = tfkl.Add()([attention_output, x1])\n    # normalize the result of the addition\n    x3 = tfkl.LayerNormalization(epsilon=1e-6)(x2)    \n    # apply a dense layer with the gelu activation function\n    x3 = tfkl.Dense(units, activation=tf.nn.gelu)(x3)\n    # apply dropout to the dense layer output\n    x3 = tfkl.Dropout(dropout_rate)(x3)\n    # add the dense layer output to the result of the addition\n    x3 = tfkl.Add()([x3, x2])\n\n    return x3, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:28:39.79077Z","iopub.execute_input":"2023-02-08T16:28:39.791593Z","iopub.status.idle":"2023-02-08T16:28:39.80446Z","shell.execute_reply.started":"2023-02-08T16:28:39.79155Z","shell.execute_reply":"2023-02-08T16:28:39.803321Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def get_visual_transformer(\n    blocks, \n    heads, \n    key_dim, \n    units, \n    num_patches, \n    projection_dim, \n    dropout_rate=0.33\n    ):\n    \n    inputs = tfkl.Input(shape=input_shape, name='inputs')\n    # Augment data\n    augmented = data_augmentation(inputs)\n    # Create patches\n    patches = Patches(patch_size)(augmented)\n    # Encode patches\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n    \n    for b in range(blocks):\n        encoded_patches, attention_weights = transformer_block(\n            encoded_patches, \n            heads, \n            key_dim, \n            units,\n            dropout_rate,\n            name='block'+str(b)+'_'\n        )\n    \n    # Create a [batch_size, projection_dim] tensor\n    representation = tfkl.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = tfkl.Flatten()(representation)\n    representation = tfkl.Dropout(0.5)(representation)\n    # Classify outputs\n    logits = tfkl.Dense(num_classes)(representation) #num_classes = dim of latent representation of image\n    # Create the Keras model\n    model = tfk.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-08T16:28:39.808097Z","iopub.execute_input":"2023-02-08T16:28:39.80878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we want is a latent representation of an image.\n\nnow the output of the encoder is (None, 256, 64) which is (bz, num of patches, dim of embeding for each patch)\n\nwe try to  ","metadata":{}},{"cell_type":"code","source":"num_classes=64\nvisual_encoder = get_visual_transformer(\n    blocks=4,\n    heads=4,\n    key_dim=projection_dim,\n    units=projection_dim,\n    projection_dim=projection_dim,\n    num_patches=num_patches\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visual_encoder.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Encoder\n","metadata":{}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(tfkl.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        # Embedding layer for the token\n        self.token_emb = tfkl.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        # Embedding layer for the position\n        self.pos_emb = tfkl.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        # Find the maximum length of the input\n        maxlen = tf.shape(x)[-1]\n        # Create a tensor with positions from 0 to maxlen-1\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        # Embed the positions\n        positions = self.pos_emb(positions)\n        # Embed the tokens\n        x = self.token_emb(x)\n        # Add the token and position embeddings\n        return x + positions\n\nclass TransformerEncoderBlock(tfkl.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.att = tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tfk.Sequential(\n            [\n                tfkl.Dense(ff_dim, activation=\"relu\"), \n                tfkl.Dense(embed_dim)\n            ]\n        )\n        self.layernorm1 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tfkl.Dropout(rate)\n        self.dropout2 = tfkl.Dropout(rate)\n        \n    def call(self, inputs, training):\n        # Self-attention\n        attn_output = self.att(inputs, inputs)\n        # Apply dropout to the attention output\n        attn_output = self.dropout1(attn_output, training=training)\n        # Add the attention output to the input and normalize\n        out1 = self.layernorm1(inputs + attn_output)\n        # Feed-forward\n        ffn_output = self.ffn(out1)\n        # Apply dropout to the feed-forward output\n        ffn_output = self.dropout2(ffn_output, training=training)\n        # Add the feed-forward output to the previous output and normalize\n        return self.layernorm2(out1 + ffn_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 64 # Embedding size for each token\nlatent_dim = 1024 # Dimention of the latent space\nnum_heads = 4 # Number of attention heads\n\nencoder_inputs = tfk.Input(shape=(sequence_length), dtype=\"int64\", name=\"encoder_inputs\")\n# Adding token and position embedding layer\nx = TokenAndPositionEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n# Adding transformer encoder block\nencoder_outputs = TransformerEncoderBlock(embed_dim, num_heads, latent_dim)(x)\n# Defining the encoder model\n# Apply global average pooling\nx = tfkl.GlobalAveragePooling1D()(encoder_outputs)\n# Apply dropout\nx = tfkl.Dropout(0.5)(x)\n# Add a dense layer with softmax activation\noutputs = tfkl.Dense(64, activation=\"relu\")(x)\ntext_encoder = tfk.Model(encoder_inputs, outputs)\n# Print the summary of the encoder model\ntext_encoder.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIP(tfk.Model):\n    def __init__(self, textEncoder, imageEncoder):\n        super().__init__()\n        self.textEncoder = textEncoder\n        self.imageEncoder = imageEncoder\n\n    def compile(self, optimizer):\n        super().compile()\n        self.optimizer = optimizer\n        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n        \n    @property\n    def metrics(self):\n        return [self.loss_tracker]        \n    \n    def call(self, features, training=False):\n        with tf.device(\"/gpu:0\"):\n            caption_embeddings = self.textEncoder(features[1], training=training)\n        with tf.device(\"/gpu:1\"):\n            image_embeddings = self.imageEncoder(features[0], training=training)\n        return caption_embeddings, image_embeddings\n    \n    def compute_loss(self, caption_embeddings, image_embeddings):\n        # logits[i][j] is the dot_similarity(caption_i, image_j).\n        logits = (\n            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n        )\n        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n        images_similarity = tf.matmul(\n            image_embeddings, image_embeddings, transpose_b=True\n        )\n        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n        captions_similarity = tf.matmul(\n            caption_embeddings, caption_embeddings, transpose_b=True\n        )\n        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n        targets = tfk.activations.softmax(captions_similarity + images_similarity)\n        # Compute the loss for the captions using crossentropy\n        captions_loss = tfk.losses.categorical_crossentropy(\n            y_true=targets, y_pred=logits, from_logits=True\n        )\n        # Compute the loss for the images using crossentropy\n        images_loss = tfk.losses.categorical_crossentropy(\n            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n        )\n        # Return the mean of the loss over the batch.\n        return (captions_loss + images_loss) / 2\n\n    def train_step(self, images_captions):\n        with tf.GradientTape() as tape:\n            # Forward pass\n            caption_embeddings, image_embeddings = self(images_captions, training=True)\n            loss = self.compute_loss(caption_embeddings, image_embeddings)\n        # Backward pass\n        \n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        \n        self.loss_tracker.update_state(loss)\n        \n        return {\"loss\": self.loss_tracker.result()}\n    \n    def test_step(self, images_captions):\n        caption_embeddings, image_embeddings = self(images_captions, training=False)\n        loss = self.compute_loss(caption_embeddings, image_embeddings)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre processing","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(path2caption,header=0, sep='\\t')\n\nvectorise = tfkl.TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n)\nvectorise.adapt(data['caption'])\n\ndef decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, image_size)\n    return img\n\ndef process_input(links,captions):\n    return decode_and_resize(path2img+links+'.jpg'), captions\n    \ndef make_dataset(data):   \n    dataset = tf.data.Dataset.from_tensor_slices((data['ID'], vectorise(data['caption'])))\n    dataset = dataset.shuffle(batch_size * 8)\n    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n    \n    return dataset\n\n\ndata = data.iloc[:100]\ntrain_dataset = make_dataset(data.iloc[:75])\nvalid_dataset = make_dataset(data.iloc[75:100])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contrast_model = CLIP(text_encoder,visual_encoder)\n\nreduce_lr = tfk.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.2, patience=3\n)\nearly_stopping = tfk.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)\noptimizer = tfk.optimizers.Adam(learning_rate)\n\ncontrast_model.compile(optimizer)\n\n\ncontrast_model.fit(train_dataset,\n    epochs=epochs,\n    validation_data=valid_dataset,\n    callbacks=[reduce_lr, early_stopping],\n)\n\nvision_encoder.save(\"vision_encoder\")\ntext_encoder.save(\"text_encoder\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"train\", \"valid\"], loc=\"upper right\")\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}